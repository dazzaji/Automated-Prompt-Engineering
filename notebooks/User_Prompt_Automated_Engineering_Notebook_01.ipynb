{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "60f7f3af",
      "metadata": {
        "id": "60f7f3af"
      },
      "source": [
        "## Setup\n",
        "\n",
        "### Note\n",
        "\n",
        "This is a function Notebook example in the DazzaJi Automated-Prompt-Engineering Repo at: https://github.com/dazzaji/Automated-Prompt-Engineering/blob/main/notebooks/User_Prompt_Automated_Engineering_Notebook_01.ipynb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dace9b3a",
      "metadata": {
        "id": "dace9b3a"
      },
      "outputs": [],
      "source": [
        "from IPython.display import Markdown"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "318da100",
      "metadata": {
        "id": "318da100"
      },
      "outputs": [],
      "source": [
        "!pip install openai cohere tiktoken prettytable"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "367a1cb5",
      "metadata": {
        "id": "367a1cb5"
      },
      "source": [
        "## Import Necessary Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b1b6fba",
      "metadata": {
        "id": "1b1b6fba"
      },
      "outputs": [],
      "source": [
        "import openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f89e3a6e",
      "metadata": {
        "id": "f89e3a6e"
      },
      "outputs": [],
      "source": [
        "\n",
        "import getpass\n",
        "\n",
        "# Get the OpenAI API key from the user\n",
        "OPENAI_API_KEY = getpass.getpass(prompt='Please enter your OpenAI API Key: ')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eb12e5ca",
      "metadata": {
        "id": "eb12e5ca"
      },
      "source": [
        "## User Prompt Generation with GPT-4 and Testing with GPT-3.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "C8ttCL8LT2pD",
      "metadata": {
        "id": "C8ttCL8LT2pD"
      },
      "outputs": [],
      "source": [
        "def gather_user_inputs():\n",
        "    explanation = input(\"Please provide a high-level explanation of what you want to accomplish with the prompt: \")\n",
        "    num_prompts = int(input(\"Enter the number of user prompts you'd like GPT-4 to generate and test with GPT-3.5: \"))\n",
        "    return explanation, num_prompts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8d1008bc",
      "metadata": {
        "id": "8d1008bc"
      },
      "outputs": [],
      "source": [
        "\n",
        "def display_results_markdown(prompts, responses):\n",
        "    output_lines = []\n",
        "    for prompt, response in zip(prompts, responses):\n",
        "        output_lines.append(f\"- **User Prompt:** {prompt}\")\n",
        "        output_lines.append(f\"  - **GPT-3.5 Output:** {response}\")\n",
        "    display(Markdown('\\n'.join(output_lines)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6044f3d",
      "metadata": {
        "id": "d6044f3d"
      },
      "outputs": [],
      "source": [
        "\n",
        "def generate_user_prompts_with_api(explanation, num_prompts):\n",
        "    # Explicitly make API calls to the v1/chat/completions endpoint for GPT-4\n",
        "    prompts = []\n",
        "    endpoint_url = \"https://api.openai.com/v1/chat/completions\"\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {OPENAI_API_KEY}\",\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"User-Agent\": \"openai-python\"\n",
        "    }\n",
        "\n",
        "    for _ in range(num_prompts):\n",
        "        data = {\n",
        "            \"model\": \"gpt-4\",\n",
        "            \"messages\": [\n",
        "                {\"role\": \"user\", \"content\": f\"Please generate a prompt that, when presented to a language model, would likely produce an answer describing the following: {explanation}. Do not provide the answer, only the prompt.\"}\n",
        "            ]\n",
        "        }\n",
        "        response = requests.post(endpoint_url, headers=headers, json=data)\n",
        "        response_json = response.json()\n",
        "        if \"choices\" in response_json:\n",
        "            message_content = response_json[\"choices\"][0][\"message\"][\"content\"]\n",
        "            prompts.append(message_content.strip())\n",
        "        else:\n",
        "            raise ValueError(f\"Unexpected API response structure: {response_json}\")\n",
        "\n",
        "    return prompts\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0f5a0b9e",
      "metadata": {
        "id": "0f5a0b9e"
      },
      "outputs": [],
      "source": [
        "\n",
        "import requests\n",
        "\n",
        "def test_user_prompts_with_chat_api_explicit(explanation, prompts):\n",
        "    # Explicitly make API calls to the v1/chat/completions endpoint for gpt-3.5-turbo\n",
        "    responses = []\n",
        "    endpoint_url = \"https://api.openai.com/v1/chat/completions\"\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {OPENAI_API_KEY}\",\n",
        "        \"Content-Type\": \"application/json\",\n",
        "        \"User-Agent\": \"openai-python\"\n",
        "    }\n",
        "\n",
        "    for prompt in prompts:\n",
        "        data = {\n",
        "            \"model\": \"gpt-3.5-turbo\",\n",
        "            \"messages\": [\n",
        "                {\"role\": \"user\", \"content\": f\"The user's goal is {explanation}. Please help the user evaluate the clarity and effectiveness of the following prompt. Assess the quality of the prompt based on its likelihood of achieving the user's stated goal. Prompt: {prompt}\"}\n",
        "            ]\n",
        "        }\n",
        "        response = requests.post(endpoint_url, headers=headers, json=data)\n",
        "        response_json = response.json()\n",
        "        message_content = response_json[\"choices\"][0][\"message\"][\"content\"]\n",
        "        responses.append(message_content.strip())\n",
        "    return responses\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b143adbc",
      "metadata": {
        "id": "b143adbc"
      },
      "outputs": [],
      "source": [
        "\n",
        "def main_markdown():\n",
        "    try:\n",
        "        print(\"Gathering User Inputs...\")\n",
        "        explanation, num_prompts = gather_user_inputs()\n",
        "        print(f\"Explanation: {explanation}\")\n",
        "        print(f\"Number of Prompts: {num_prompts}\")\n",
        "\n",
        "        print(\"Generating User Prompts with GPT-4...\")\n",
        "        prompts = generate_user_prompts_with_api(explanation, num_prompts)\n",
        "        print(f\"Generated Prompts: {prompts}\")\n",
        "\n",
        "        print(\"Testing User Prompts with GPT-3.5-turbo...\")\n",
        "        responses = test_user_prompts_with_chat_api_explicit(explanation, prompts)\n",
        "        print(f\"Responses: {responses}\")\n",
        "\n",
        "        print(\"Displaying Results...\")\n",
        "        display_results_markdown(prompts, responses)\n",
        "        print(\"Process Completed Successfully!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"Error Encountered!\")\n",
        "        print(str(e))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4d2c596f",
      "metadata": {
        "id": "4d2c596f"
      },
      "source": [
        "To run the workflow, simply execute the cell below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2798a74e",
      "metadata": {
        "id": "2798a74e"
      },
      "outputs": [],
      "source": [
        "main_markdown()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}